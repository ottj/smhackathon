{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import fastText as ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine as cosine_dist\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ft.load_model(\"../embeddings/fasttext/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('dataset/dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, corpus):\n",
    "    \"\"\" For now just based on sum of all words embedings\"\"\"\n",
    "    corpus = corpus.replace('\\n', '').split()\n",
    "    a = [0] * len(model.get_word_vector('0'))\n",
    "    for w in corpus:\n",
    "        a += model.get_word_vector(w)/len(corpus)\n",
    "    return a\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_db = []\n",
    "for i in range(db['text'].count()):\n",
    "    corpus = db['text'][i] #.replace('\\n', '').split()\n",
    "    emb_db.append(get_embedding(model, corpus))\n",
    "    \n",
    "pd.DataFrame(emb_db).to_csv('database/song_embedings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Single ladies\"\n",
    "ind = []\n",
    "for song in emb_db:\n",
    "    ind.append(cosine_dist(get_embedding(model,sentence), song))\n",
    "    \n",
    "ind = np.argmax(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single ladies\n",
      "Answer Me\n",
      "Frankie Laine\n",
      "('ANSWER ME OH MY LOVE  \\n'\n",
      " 'JUST WHAT SIN HAVE I BEEN GUILTY OF  \\n'\n",
      " 'TELL ME HOW I CAME TO LOSE YOUR LOVE  \\n'\n",
      " 'PLEASE ANSWER ME MY LOVE  \\n'\n",
      " '  \\n'\n",
      " 'YOU WERE MINE YESTERDAY  \\n'\n",
      " 'I BELIEVED THAT LOVE WAS HERE TO STAY  \\n'\n",
      " \"WON'T YOU TELL ME WHERE I'VE GONE ASTRAY  \\n\"\n",
      " 'PLEASE ANSWER ME MY LOVE  \\n'\n",
      " '  \\n'\n",
      " \"IF YOU'RE HAPPIER WITHOUT ME  \\n\"\n",
      " \"I'LL TRY NOT TO CARE  \\n\"\n",
      " 'BUT IF YOU STILL THINK ABOUT ME  \\n'\n",
      " 'PLEASE LISTEN TO MY PRAYER  \\n'\n",
      " '  \\n'\n",
      " \"YOU MUST KNOW I'VE BEEN TRUE  \\n\"\n",
      " \"WON'T YOU SAY THAT WE CAN START ANEW  \\n\"\n",
      " 'IN MY SORROW NOW I TURN TO YOU  \\n'\n",
      " 'PLEASE ANSWER ME MY LOVE\\n'\n",
      " '\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(db.ix[ind]['song'])\n",
    "print(db.ix[ind]['artist'])\n",
    "pprint(db.ix[ind]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, DropoutWrapper, MultiRNNCell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORFLOW STUFF STARTS HERE\n",
    "\n",
    "seq_len = 1\n",
    "gamma = 0.95  # Discount rate for future actions (Bellman equation)\n",
    "epsilon = 1.0  # Exploration rate (E-greedy)\n",
    "epsilon_min = 0.01\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "\n",
    "INPUT_DIM = 300  # EMBEDING SIZE (300?) ?\n",
    "OUTPUT_DIM = 300 \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "\n",
    "\n",
    "\n",
    "    # Define the input data of the model\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, None, INPUT_DIM))\n",
    "    targets = tf.placeholder(tf.float32, shape=(None, OUTPUT_DIM))\n",
    "    batch_size = 1 # tf.shape(inputs)[1]\n",
    "\n",
    "\n",
    "    def dropout_lstm(n=100, dropout=0.5):\n",
    "        lstm = LSTMCell(n, state_is_tuple=True)\n",
    "        lstm_drop = DropoutWrapper(lstm, output_keep_prob= 1.0 - dropout)\n",
    "        return lstm_drop\n",
    "\n",
    "    # Mulity layered RNN by LSTM cells\n",
    "    with tf.name_scope('lstm'):\n",
    "        lstm_stack = MultiRNNCell([dropout_lstm(i) for i in [1000,150,50]]) # Fucking chosen at random\n",
    "        initial_state = lstm_stack.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope('rnn'):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm_stack, inputs, initial_state = initial_state)\n",
    "        outputs = outputs[:,-1] # We only need the last output tensor to pass into the fully connected layer\n",
    "\n",
    "    with tf.name_scope('dense'):\n",
    "        predictions = tf.layers.dense(outputs, OUTPUT_DIM)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        # Cost function and optimizer\n",
    "        cost = tf.losses.mean_squared_error(predictions, targets)\n",
    "\n",
    "    # Gradient clipping\n",
    "    with tf.name_scope('train'):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients = train_op.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "        optimizer = train_op.apply_gradients(capped_gradients)\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Tensorboard summary writter\n",
    "    writer = tf.summary.FileWriter('log', graph=tf.get_default_graph())\n",
    "\n",
    "    # Variables initializer\n",
    "    init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cld2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57650 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 152, 300) for Tensor 'Placeholder_1:0', which has shape '(?, 300)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-461f8f770b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtarget_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtarget_word\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#         print(sess.run([cost],  feed_dict={inputs : word, targets : target_word}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1113\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1114\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 152, 300) for Tensor 'Placeholder_1:0', which has shape '(?, 300)'"
     ]
    }
   ],
   "source": [
    "def get_single_embeding(model, corpus):\n",
    "    \"\"\" For now just based on sum of all words embedings\"\"\"\n",
    "    corpus = corpus.replace('\\n', '').split()\n",
    "    a = []\n",
    "    for w in corpus:\n",
    "        a.append(model.get_word_vector(w))\n",
    "    return a\n",
    "        \n",
    "down = db.shape[0]\n",
    "for j in tqdm(range(200)):\n",
    "    text = db['text'][j]\n",
    "\n",
    "    corpus = get_single_embeding(model, text)\n",
    "    # saver = tf.train.Saver() -> TO DO: Implement model saver/loader\n",
    "    sess = tf.Session(graph=graph)\n",
    "    sess.run(init_op)\n",
    "\n",
    "#     print(len(corpus))\n",
    "    for i in range(len(corpus)-1):\n",
    "        word = [corpus[:-1]]\n",
    "        target_word = [corpus[1:]]\n",
    "        _, summary = sess.run([optimizer, summary_op], feed_dict={inputs : word, targets : target_word})\n",
    "#         print(sess.run([cost],  feed_dict={inputs : word, targets : target_word}))\n",
    "        writer.add_summary(summary, i)\n",
    "#     a = sess.run([final_state],  feed_dict={inputs : word, targets : target_word})\n",
    "#     if j % 100:\n",
    "#         print(sess.run([cost],  feed_dict={inputs : word, targets : target_word}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find = input()\n",
    "\n",
    "word = [[to_find[:-1]]]\n",
    "target_word = [to_find[1:]]\n",
    "baseline_check = sess.run([final_state], feed_dict={inputs : word, targets : target_word})\n",
    "\n",
    "IND = []\n",
    "for i in range(len(corpus)-1):\n",
    "    word = [[corpus[i]]]\n",
    "    target_word = [corpus[i+1]]\n",
    "    final_state = sess.run([final_state], feed_dict={inputs : word, targets : target_word})\n",
    "    IND.append(cosine_dist(get_embedding(model,sentence), song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(IND)\n",
    "print(to_find)\n",
    "print(db.ix[ind]['song'])\n",
    "print(db.ix[ind]['artist'])\n",
    "pprint(db.ix[ind]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
